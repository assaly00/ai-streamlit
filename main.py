import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_teddynote.prompts import load_prompt
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_teddynote import logging
from dotenv import load_dotenv
import os
# PDF íŒŒì‹±ê³¼ ê´€ë ¨ëœ í•„ìˆ˜ ëª¨ë“ˆë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤
from layoutparse.teddynote_parser import (
    create_upstage_parser_graph,  # Upstage íŒŒì„œ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    create_export_graph,  # ê²°ê³¼ë¥¼ ë‚´ë³´ë‚´ëŠ” ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
)

# ê·¸ë˜í”„ ê´€ë ¨ ê¸°ë³¸ ëª¨ë“ˆë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤
from langgraph.graph import StateGraph, END  # ìƒíƒœ ê·¸ë˜í”„ì™€ ì¢…ë£Œ ìƒíƒœë¥¼ ìœ„í•œ ëª¨ë“ˆ
from layoutparse.state import ParseState  # íŒŒì‹± ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
from langchain_teddynote.graphs import visualize_graph  # ê·¸ë˜í”„ ì‹œê°í™”ë¥¼ ìœ„í•œ ëª¨ë“ˆ
from langgraph.checkpoint.memory import MemorySaver  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì„ ìœ„í•œ ëª¨ë“ˆ
# UUID ëª¨ë“ˆì„ ê°€ì ¸ì™€ì„œ ê³ ìœ  ì‹ë³„ìë¥¼ ìƒì„±í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤
import uuid
# Langchainì˜ ì‹¤í–‰ ì„¤ì •ì„ ìœ„í•œ RunnableConfigë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
from langchain_core.runnables import RunnableConfig
# ê·¸ë˜í”„ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ê¸° ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
#from langchain_teddynote.messages import stream_graph
from langgraph.graph.state import CompiledStateGraph
from typing import Any, Dict, List, Callable
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser


# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
logging.langsmith("[Project] Upstage PDF Parser")

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    print("Create .cache directory")
    os.mkdir(".cache")

# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”
if not os.path.exists(".cache/files"):
    print("Create .cache/files directory")
    os.mkdir(".cache/files")

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

st.set_page_config(page_title="LLM Playground ğŸ’¬", page_icon="ğŸ’¬")
st.title("AI OCR - Upstage LLM ê¸°ë°˜ ğŸ’¬")


########################################################################
# LangGraph ì „ì²´ ì›Œí¬í”Œë¡œìš° êµ¬ì„±
########################################################################
# Upstage íŒŒì„œ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ - ë°°ì¹˜ í¬ê¸° 30ìœ¼ë¡œ ì„¤ì •í•˜ê³  ìƒì„¸ ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤
upstage_parser_graph = create_upstage_parser_graph(
    batch_size=30, test_page=None, verbose=True
)
# ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë‚´ë³´ë‚´ëŠ” ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
export_graph = create_export_graph(show_image_in_markdown=True)

# ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹´ì„ ë¶€ëª¨ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
parent_workflow = StateGraph(ParseState)

# Upstage íŒŒì„œ ë…¸ë“œë¥¼ ì›Œí¬í”Œë¡œìš°ì— ì¶”ê°€í•©ë‹ˆë‹¤
parent_workflow.add_node("upstage_parser", upstage_parser_graph)
# ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ë…¸ë“œë¥¼ ì›Œí¬í”Œë¡œìš°ì— ì¶”ê°€í•©ë‹ˆë‹¤
parent_workflow.add_node("export_output", export_graph)

# íŒŒì„œì—ì„œ ë‚´ë³´ë‚´ê¸°ë¡œ ì´ì–´ì§€ëŠ” ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤
parent_workflow.add_edge("upstage_parser", "export_output")

# ì›Œí¬í”Œë¡œìš°ì˜ ì‹œì‘ì ì„ Upstage íŒŒì„œë¡œ ì„¤ì •í•©ë‹ˆë‹¤
parent_workflow.set_entry_point("upstage_parser")

# ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•©ë‹ˆë‹¤
parent_graph = parent_workflow.compile(checkpointer=MemorySaver())

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤
prompt_content = ""
html_content = ""
markdown_content = ""
ocrjson_content = ""

########################################################################


# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.
    st.session_state["messages"] = []

if "chain" not in st.session_state:
    # ì•„ë¬´ëŸ° íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ì§€ ì•Šì„ ê²½ìš°
    st.session_state["chain"] = None


# íƒ­ì„ ìƒì„±
html_tab, markdown_tab, convjson_tab, message_tab = st.tabs(["HTML", "Markdown", "ë³€í™˜JSON", "ëŒ€í™”ë‚´ìš©"])

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    tab1, tab2 = st.tabs(["LLM OCR", "JSON ë³€í™˜"])

    ## LLM OCR íƒ­
    # íŒŒì¼ ì—…ë¡œë“œ
    tab1.uploaded_file = tab1.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

    ## JSON ë³€í™˜ íƒ­
    # ëª¨ë¸ ì„ íƒ ë©”ë‰´
    tab2.selected_model = tab2.selectbox(
        "LLM ì„ íƒ", ["gpt-3.5-turbo", "gpt-4o", "gpt-4-turbo", "gpt-4o-mini"], index=0
    ) 
 
    user_selected_prompt = tab2.selectbox("JSONë³€í™˜ í”„ë¡¬í”„íŠ¸ ì„ íƒ", ["invoice-latest", "chinese-test"])
    translate_btn = tab2.button("JSON ë³€í™˜", key="apply")
        

# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for message_tab.chat_message in st.session_state["messages"]:
        message_tab.chat_message(message_tab.chat_message.role).write(message_tab.chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))

def stream_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: RunnableConfig,
    node_names: List[str] = [],
    callback: Callable = None,
):
    """
    LangGraphì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        graph (CompiledStateGraph): ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs (dict): ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬
        config (RunnableConfig): ì‹¤í–‰ ì„¤ì •
        node_names (List[str], optional): ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡. ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸
        callback (Callable, optional): ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜. ê¸°ë³¸ê°’ì€ None
            ì½œë°± í•¨ìˆ˜ëŠ” {"node": str, "content": str} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.

    Returns:
        None: í•¨ìˆ˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì¶œë ¥ë§Œ í•˜ê³  ë°˜í™˜ê°’ì€ ì—†ìŠµë‹ˆë‹¤.
    """
    print("\nğŸš€ LangGraph ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤ ğŸš€")
    
    prev_node = ""
    for chunk_msg, metadata in graph.stream(inputs, config, stream_mode="messages"):
        curr_node = metadata["langgraph_node"]

        # node_namesê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜„ì¬ ë…¸ë“œê°€ node_namesì— ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if not node_names or curr_node in node_names:
            # ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‹¤í–‰
            if callback:
                callback({"node": curr_node, "content": chunk_msg.content})
            # ì½œë°±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¶œë ¥
            else:
                # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥
                if curr_node != prev_node:
                    # add_message("assistant", "\n" + "=" * 50)
                    # add_message("assistant", f"ğŸ”„ Node: \033[1;36m{curr_node}\033[0m ğŸ”„")
                    # add_message("assistant", "- " * 25)
                    print("\n" + "=" * 50)
                    print(f"ğŸ”„ Node: \033[1;36m{curr_node}\033[0m ğŸ”„")
                    print("- " * 25)
                print(chunk_msg.content, end="", flush=True)
                #add_message("assistant", chunk_msg.content, end="", flush=True)

            prev_node = curr_node

# ì „ì—­ ë³€ìˆ˜ ì„ ì–¸
config = None
@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ì„ ë¶„ì„ì¤‘ì…ë‹ˆë‹¤...")
def call_upstage_parser(file):
    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.    
    file_content = file.read()
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ì ˆëŒ€ ê²½ë¡œ ì–»ê¸°
    current_directory = os.path.abspath("")
    # íŒŒì¼ ê²½ë¡œ ìƒì„±
    file_path = os.path.join(current_directory, ".cache", "files", file.name)
    #file_path = f"./.cache/files/{file.name}"
    #file_path = f"D:/vs-workspace/langchain-kr/19-Streamlit/01-MyProject/.cache/files/{file.name}"    
    with open(file_path, "wb") as f:
        f.write(file_content)
    

    print(f"filepath type: {type(file_path)}, value: {file_path}")

    # ê·¸ë˜í”„ ì‹¤í–‰ì„ ìœ„í•œ ìƒì„¸ ì„¤ì •ì„ êµ¬ì„±í•©ë‹ˆë‹¤
    config = RunnableConfig(
        # ì¬ê·€ í˜¸ì¶œì˜ ìµœëŒ€ ê¹Šì´ë¥¼ 300ìœ¼ë¡œ ì œí•œí•˜ì—¬ ë¬´í•œ ë£¨í”„ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤
        recursion_limit=300,
        # ì‹¤í–‰ë§ˆë‹¤ ê³ ìœ í•œ ìŠ¤ë ˆë“œ IDë¥¼ ìƒì„±í•˜ì—¬ ë…ë¦½ì ì¸ ì‹¤í–‰ì„ ë³´ì¥í•©ë‹ˆë‹¤
        configurable={"thread_id": str(uuid.uuid4())},
    )

    # ë¶„ì„í•  PDF íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ê°’ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤
    inputs = {
        # PDF íŒŒì¼ì˜ ì‹¤ì œ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤        
        "filepath": {file_path},
    }

    print(inputs)

    # ì„¤ì •ëœ ê·¸ë˜í”„ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤
    stream_graph(parent_graph, inputs, config=config)    
    return


# ì²´ì¸ ìƒì„±
def create_chain(model_name="gpt-4o"):
    # ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)
    # í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    prompt = load_prompt("prompts/pdf-rag.yaml", encoding="utf-8")

    # ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±
    # ëª¨ë¸(LLM) ì„ ìƒì„±í•©ë‹ˆë‹¤.
    llm = ChatOpenAI(model_name=model_name, temperature=0)

    # ë‹¨ê³„ 8: ì²´ì¸(Chain) ìƒì„±
    chain = (
        #{"context": markdown_content, "question": RunnablePassthrough()}
        {"context": StrOutputParser().pipe(lambda x: markdown_content), "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain


# JSONë³€í™˜ ì²´ì¸ ìƒì„±
@st.cache_resource(show_spinner="ì§€ì •í•œ JSONí¬ë§·ìœ¼ë¡œ ë³€í™˜ì¤‘ì…ë‹ˆë‹¤...")
def exec_trans_chain(_prompt, model_name="gpt-4o"):
    # JSON ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™”
    parser = JsonOutputParser()
    # ChatGPT ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì„¤ì •í•©ë‹ˆë‹¤
    llm = ChatOpenAI(model=model_name, temperature=0)
    # í”„ë¡¬í”„íŠ¸ì™€ LLMì„ ì²´ì¸ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤
    chain = _prompt | llm | parser
    # ì§€ì‹œì‚¬í•­ì„ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•©ë‹ˆë‹¤.
    _prompt = _prompt.partial(format_instructions=parser.get_format_instructions())        
    # ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ ì¿¼ë¦¬ ì‹¤í–‰
    response = chain.invoke({"ocr_data": markdown_content, "format_instructions": "Return a JSON object"})

    with convjson_tab:
        convjson_tab.json(response)

    # ìƒˆë¡œìš´ íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆìœ¼ë¯€ë¡œ ì±„íŒ…ë‚´ìš© 
    # ì‚­ì œ
    st.session_state["messages"] = []
    return response


# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ
if tab1.uploaded_file:
    # íŒŒì¼ ì—…ë¡œë“œ í›„ retriever ìƒì„± (ì‘ì—…ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ì˜ˆì •...)
    #retriever = embed_file(uploaded_file)
    call_upstage_parser(tab1.uploaded_file)
    #chain = create_chain(retriever, model_name=selected_model)
    #st.session_state["chain"] = chain

    filename = os.path.abspath("") + "/.cache/files/" + os.path.splitext(tab1.uploaded_file.name)[0]    
    # ocrjson_file_path = filename + "_0000_0000.json"
    # #ocrjson_file_path = f"D:/vs-workspace/langchain-kr/19-Streamlit/01-MyProject/.cache/files/Terminal Invoice Samples10_0000_0000.json"            
    # with open(ocrjson_file_path, "r", encoding="utf-8") as file:
    #     ocrjson_content = file.read()
    #     with ocrjson_tab:
    #         st.json(ocrjson_content)

    #html_file_path = f"D:/vs-workspace/langchain-kr/19-Streamlit/01-MyProject/.cache/files/Terminal Invoice Samples10.html"
    html_file_path = filename + ".html"
    with open(html_file_path, "r", encoding="utf-8") as file:
        html_content = file.read()
        with html_tab:
            html_tab.write(html_content, unsafe_allow_html=True)

    #markdown_file_path = f"D:/vs-workspace/langchain-kr/19-Streamlit/01-MyProject/.cache/files/Terminal Invoice Samples10.md"            
    markdown_file_path = filename + ".md"
    with open(markdown_file_path, "r", encoding="utf-8") as file:
        markdown_content = file.read()
        with markdown_tab:
            markdown_tab.markdown(markdown_content)

    # íŒŒì¼ ì—…ë¡œë“œ í›„ chain ìƒì„±
    chain = create_chain(model_name=tab2.selected_model)
    st.session_state["chain"] = chain                
    tab1.markdown(f"âœ… OCR ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")    




# í”„ë¡¬í”„íŠ¸ ì ìš© ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...
if translate_btn:    
    prompt = load_prompt(f"prompts/ocr-{user_selected_prompt}.yaml", encoding="utf8")
    exec_trans_chain(prompt, model_name=tab2.selected_model)
    # ì´ì „ ìƒíƒœì˜ ê·¸ë˜í”„ ê°’ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
    #previous_state = upstage_parser_graph.get_state(config).values
    # íŒŒì„œê°€ ì¶”ì¶œí•œ ìš”ì†Œë“¤ ì¤‘ ë§ˆì§€ë§‰ 30ê°œ ìš”ì†Œì—ì„œ 5ê°œë§Œ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ í™•ì¸í•©ë‹ˆë‹¤ (ë””ë²„ê¹… ëª©ì )
    #previous_state["elements_from_parser"][-30:-25]
    #previous_state["elements_from_parser"]
    tab2.markdown(f"âœ… JSON ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")    
   
    

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = message_tab.chat_input("ì—…ë¡œë“œí•œ íŒŒì¼ì— ëŒ€í•´ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...
if clear_btn:
    st.session_state["messages"] = []
    
# ë§Œì•½ì— ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...
if user_input:
    # chain ì„ ìƒì„±
    chain = st.session_state["chain"]

    if chain is not None:
        # ì‚¬ìš©ìì˜ ì…ë ¥
        message_tab.chat_message("user").write(user_input)
        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        response = chain.stream(user_input)
        with message_tab.chat_message("assistant"):
            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
            container = st.empty()

            ai_answer = ""
            for token in response:
                ai_answer += token
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
        add_message("user", user_input)
        add_message("assistant", ai_answer)
    else:
        # íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ë¼ëŠ” ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
        warning_msg.error("íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")
